{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "hate-speeches_editado_vinicius.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "xNNXZzZI52Nk",
        "G0E3Qsf452Nk",
        "Hy1BPTtU52Nl",
        "efybKhii52Nm",
        "ZcCBtts152Nn",
        "da-yLe2O52No",
        "ru7slR3q52Ns",
        "LGKc8Ep-52Nx",
        "qNqXzGQY52N5",
        "_dv-3TNX52N9",
        "KBfFn8Iw52N_",
        "KB7FEjwt52OB",
        "VKgzvaQM6lPl"
      ]
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_EZu2QuL52Nc",
        "colab_type": "text"
      },
      "source": [
        "<p style='text-align: center;'> \n",
        "    <img src='./assets/logotipo_facens.jpg'/>\n",
        "</p>\n",
        "<hr />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "26LAC7tl52Nd",
        "colab_type": "text"
      },
      "source": [
        "<h1 align='center'><b>Faculdade de Engenharia de Sorocaba</b></h1>\n",
        "<h3 align='center'><i>Especialização em Ciência de Dados</i></h3>\n",
        "<hr />\n",
        "<h3 align='center'>Adriano Valério Santos da Silva</h3>\n",
        "<h3 align='center'>Allan Flores de Jesus</h3> \n",
        "<h3 align='center'>Vinícius Targa Gonçalves</h3> "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LP__6unD52Nd",
        "colab_type": "text"
      },
      "source": [
        "# "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VYiQorpL52Ne",
        "colab_type": "text"
      },
      "source": [
        "<p style='text-align: center; font-size: 60px;'> \n",
        "    <b>HATE SPEECHES</b>\n",
        "</p>\n",
        "<hr />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VbttHyME52Ne",
        "colab_type": "text"
      },
      "source": [
        "<h1><b>AGRADECIMENTOS</b></h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ab6vovyz52Nf",
        "colab_type": "text"
      },
      "source": [
        "(conteúdo)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6nfvPTGT52Nf",
        "colab_type": "text"
      },
      "source": [
        "# "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ed0wNBqF52Ng",
        "colab_type": "text"
      },
      "source": [
        "<h1><b>RESUMO</b></h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LIwVBZJP52Ng",
        "colab_type": "text"
      },
      "source": [
        "(conteúdo)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U81lUIcK52Nh",
        "colab_type": "text"
      },
      "source": [
        "# "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "egip1Gu052Nh",
        "colab_type": "text"
      },
      "source": [
        "<h1><b>INTRODUÇÃO</b></h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mF0CnSII52Nh",
        "colab_type": "text"
      },
      "source": [
        "(conteúdo)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I5COKIN552Ni",
        "colab_type": "text"
      },
      "source": [
        "# "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_nzR2vJB52Ni",
        "colab_type": "text"
      },
      "source": [
        "<h1><b>SUMÁRIO</b></h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LslOj-5l52Nj",
        "colab_type": "text"
      },
      "source": [
        "**[1. Contextualização](#n1_contextualizacao)**\n",
        "  * [1.1 Problemas identificados](#n2_problemas_identificados)\n",
        "  * [1.2 Objetivos](#n2_objetivos)\n",
        "  * [1.3 Dataset escolhido](#n2_dataset_escolhido)\n",
        "  * [1.4 Pesquisas Relacionadas](#n2_pesquisas_relacionadas)\n",
        "  * [1.5 Continuidade do projeto](#n2_continuidade_projeto)\n",
        "    \n",
        "**[2. Projeto](#n1_projeto)**\n",
        "  * [2.1 Importação das bibliotecas](#n2_importacao_bibliotecas)\n",
        "  * [2.2 Instalação de dependências](#n2_instalacao_dependencias)\n",
        "  * [2.3 Importação do dataset](#n2_importacao_dataset)\n",
        "    * [2.3.1 Head](#n3_head)\n",
        "    * [2.3.2 Dimensionalidade](#n3_dimensionalidade)\n",
        "    * [2.3.3 Tipo dos campos](#n3_tipos_campos)\n",
        "  * [2.4 Análise exploratória](#n2_analise_exploratoria)\n",
        "    * [2.4.1 Contagem de sentenças por classificação](#n3_contagem_classificacao)\n",
        "    * [2.4.2 Redefinição da nomenclatura das colunas](#n3_redefinicao_colunas)\n",
        "    * [2.4.3 Redefinição da coluna de classificação (0 e 1)](#n3_redefinicao_coluna_classificacao)\n",
        "    * [2.4.4 Palavras mais comuns por categoria](#n3_palavras_comuns)\n",
        "    * [2.4.5 Frequência das palavras](#n3_frequencia_das_palavras)\n",
        "  * [2.5 Técnicas de Pré Processamento](#n2_tecnicas_pre_processamento)\n",
        "    * [2.5.1 Workflow NLP](#n3_workflow_nlp)\n",
        "    * [2.5.2 Removendo stop words e pontuação](#n3_removendo_stop_words)\n",
        "    * [2.5.3 Pontuacao](#n3_pontuacao)\n",
        "    * [2.5.4 Normalização do texto](#n3_normalizacao_texto)\n",
        "    * [2.5.5 Caracteres maiúsculos](#n3_caracteres_maiusculo)\n",
        "    * [2.5.6 Stemming](#n3_stemming)\n",
        "    * [2.5.7 Word Cloud](#n3_word_cloud)\n",
        "    * [2.5.8 Frequencia das Palavras (Gráfico de Pareto)](#n3_frequencia_palavras_pareto)   \n",
        "  * [2.6 Classificadores](#n2_classificadores)\n",
        "    * [2.6.1 TF-IDF / Balanceamento de Dados](#n3_tf_idf_balanceamento)\n",
        "    * [2.6.2 Global Class](#n3_global_class)\n",
        "    * [2.6.3 Logistic Regression](#n3_logistic_regression)\n",
        "    * [2.6.4 Gaussian Naive Bayes](#n3_gaussian_nb)\n",
        "    * [2.6.5 Multinomial NB](#n3_multinomial_nb)\n",
        "    * [2.6.6 Random Forest](#n3_random_forest)\n",
        "    * [2.6.7 (...)](#n3_)   \n",
        "  * [2.7 Geração dos modelos (armazenamento)](#n2_geracao_modelos)\n",
        "  * [2.8 SPA de classificação](#n2_spa_classificacao)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AtXTuOHR52Nj",
        "colab_type": "text"
      },
      "source": [
        "<hr />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xNNXZzZI52Nk",
        "colab_type": "text"
      },
      "source": [
        "## 1. Contextualização <a name=\"n1_contextualizacao\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G0E3Qsf452Nk",
        "colab_type": "text"
      },
      "source": [
        "### 1.1. Problemas Identificados <a name=\"n2_problemas_identificados\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lrAsACUu52Nk",
        "colab_type": "text"
      },
      "source": [
        "(conteúdo)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hy1BPTtU52Nl",
        "colab_type": "text"
      },
      "source": [
        "### 1.2. Objetivos <a name=\"n2_objetivos\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VZ95dfoJ52Nl",
        "colab_type": "text"
      },
      "source": [
        "(conteúdo)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "efybKhii52Nm",
        "colab_type": "text"
      },
      "source": [
        "### 1.3. Dataset escolhido <a name=\"n2_dataset_escolhido\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G4ENV3_R52Nm",
        "colab_type": "text"
      },
      "source": [
        "(conteúdo)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mHZsARmb52Nm",
        "colab_type": "text"
      },
      "source": [
        "### 1.4. Pesquisas relacionadas <a name=\"n2_pesquisas_relacionadas\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xHlYuscU52Nn",
        "colab_type": "text"
      },
      "source": [
        "(conteúdo)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZcCBtts152Nn",
        "colab_type": "text"
      },
      "source": [
        "### 1.5. Continuidade do projeto <a name=\"n2_continuidade_projeto\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EvA3FmoN52No",
        "colab_type": "text"
      },
      "source": [
        "(conteúdo)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Y-XMzOF52No",
        "colab_type": "text"
      },
      "source": [
        "## 2. Projeto <a name=\"n1_projeto\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "da-yLe2O52No",
        "colab_type": "text"
      },
      "source": [
        "### 2.1. Importação das bibliotecas <a name=\"n2_importacao_bibliotecas\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kRWigOFK52Np",
        "colab_type": "code",
        "colab": {},
        "outputId": "99f45814-0f0b-460d-abfb-8d02eea31a97"
      },
      "source": [
        "# Bibliotecas:\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import nltk\n",
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import scipy\n",
        "import string\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "from nltk.probability import FreqDist\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.stem import SnowballStemmer\n",
        "from nltk.corpus import wordnet\n",
        "from collections import  Counter\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn import neighbors\n",
        "from sklearn.metrics import *\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from nltk.corpus import stopwords\n",
        "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
        "\n",
        "##################################################################################\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import string\n",
        "from collections import  Counter\n",
        "import itertools\n",
        "\n",
        "# NLTK\n",
        "import nltk\n",
        "from nltk.tokenize import TweetTokenizer, word_tokenize\n",
        "from nltk import pos_tag\n",
        "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
        "from nltk.corpus import wordnet, stopwords\n",
        "from nltk.tokenize.treebank import TreebankWordTokenizer\n",
        "\n",
        "# SCYKITLEARN\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB, GaussianNB\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn import preprocessing\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "# KERAS\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from keras.layers import Dropout, Embedding, SpatialDropout1D, LSTM, Dense\n",
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "# Tensorflow\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import regularizers\n",
        "import tensorflow as tf\n",
        "\n",
        "# Plot\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Download NLTK files\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     /Users/adrianovss/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     /Users/adrianovss/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw to /Users/adrianovss/nltk_data...\n",
            "[nltk_data]   Package omw is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /Users/adrianovss/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     /Users/adrianovss/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ru7slR3q52Ns",
        "colab_type": "text"
      },
      "source": [
        "### 2.2. Instalação das dependências <a name=\"n2_instalacao_dependencias\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0K_yyfaH52Nt",
        "colab_type": "code",
        "colab": {},
        "outputId": "7d68d986-8d22-48ab-9dc7-af47366a154a"
      },
      "source": [
        "!pip install tensorflow\n",
        "!pip install keras"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow in /opt/anaconda3/lib/python3.8/site-packages (2.3.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /opt/anaconda3/lib/python3.8/site-packages (from tensorflow) (0.10.0)\n",
            "Requirement already satisfied: scipy==1.4.1 in /opt/anaconda3/lib/python3.8/site-packages (from tensorflow) (1.4.1)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /opt/anaconda3/lib/python3.8/site-packages (from tensorflow) (3.13.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /opt/anaconda3/lib/python3.8/site-packages (from tensorflow) (1.11.2)\n",
            "Requirement already satisfied: gast==0.3.3 in /opt/anaconda3/lib/python3.8/site-packages (from tensorflow) (0.3.3)\n",
            "Requirement already satisfied: tensorflow-estimator<2.4.0,>=2.3.0 in /opt/anaconda3/lib/python3.8/site-packages (from tensorflow) (2.3.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /opt/anaconda3/lib/python3.8/site-packages (from tensorflow) (0.34.2)\n",
            "Requirement already satisfied: google-pasta>=0.1.8 in /opt/anaconda3/lib/python3.8/site-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: astunparse==1.6.3 in /opt/anaconda3/lib/python3.8/site-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: keras-preprocessing<1.2,>=1.1.1 in /opt/anaconda3/lib/python3.8/site-packages (from tensorflow) (1.1.2)\n",
            "Requirement already satisfied: tensorboard<3,>=2.3.0 in /opt/anaconda3/lib/python3.8/site-packages (from tensorflow) (2.3.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /opt/anaconda3/lib/python3.8/site-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied: numpy<1.19.0,>=1.16.0 in /opt/anaconda3/lib/python3.8/site-packages (from tensorflow) (1.18.5)\n",
            "Requirement already satisfied: six>=1.12.0 in /opt/anaconda3/lib/python3.8/site-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /opt/anaconda3/lib/python3.8/site-packages (from tensorflow) (2.10.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /opt/anaconda3/lib/python3.8/site-packages (from tensorflow) (1.32.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/anaconda3/lib/python3.8/site-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.8/site-packages (from protobuf>=3.9.2->tensorflow) (49.2.0.post20200714)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /opt/anaconda3/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow) (1.21.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /opt/anaconda3/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow) (3.2.2)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/anaconda3/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /opt/anaconda3/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow) (1.7.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /opt/anaconda3/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow) (1.0.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /opt/anaconda3/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow) (2.24.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/anaconda3/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (4.1.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/anaconda3/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.5\" in /opt/anaconda3/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (4.6)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/anaconda3/lib/python3.8/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow) (1.3.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/anaconda3/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/anaconda3/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (1.25.9)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /opt/anaconda3/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (2020.6.20)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/anaconda3/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /opt/anaconda3/lib/python3.8/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow) (3.1.0)\n",
            "Collecting keras\n",
            "  Downloading Keras-2.4.3-py2.py3-none-any.whl (36 kB)\n",
            "Requirement already satisfied: pyyaml in /opt/anaconda3/lib/python3.8/site-packages (from keras) (5.3.1)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /opt/anaconda3/lib/python3.8/site-packages (from keras) (1.18.5)\n",
            "Requirement already satisfied: h5py in /opt/anaconda3/lib/python3.8/site-packages (from keras) (2.10.0)\n",
            "Requirement already satisfied: scipy>=0.14 in /opt/anaconda3/lib/python3.8/site-packages (from keras) (1.4.1)\n",
            "Requirement already satisfied: six in /opt/anaconda3/lib/python3.8/site-packages (from h5py->keras) (1.15.0)\n",
            "Installing collected packages: keras\n",
            "Successfully installed keras-2.4.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RoabFqbG52Nv",
        "colab_type": "code",
        "colab": {},
        "outputId": "0f721378-c671-4294-938b-60f405fdf47a"
      },
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     /Users/adrianovss/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     /Users/adrianovss/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw to /Users/adrianovss/nltk_data...\n",
            "[nltk_data]   Package omw is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /Users/adrianovss/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     /Users/adrianovss/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LGKc8Ep-52Nx",
        "colab_type": "text"
      },
      "source": [
        "### 2.3. Importação do dataset <a name=\"n2_importacao_dataset\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PVWIhG9c52Nx",
        "colab_type": "text"
      },
      "source": [
        "#### 2.3.1 Head <a name=\"n3_head\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xc9hfpOi52Ny",
        "colab_type": "code",
        "colab": {},
        "outputId": "08f18402-0fff-485e-b07c-685299db68d9"
      },
      "source": [
        "df = pd.read_csv('./datasets/offensive_speeches.csv')\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>@@class</th>\n",
              "      <th>document</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>yes</td>\n",
              "      <td>Votaram no PEZAO Agora tomem no CZAO</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>no</td>\n",
              "      <td>cuidado com a poupanca pessoal Lembram o que a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>no</td>\n",
              "      <td>Sabe o que eu acho engracado os nossos governa...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>yes</td>\n",
              "      <td>os cariocas tem o que merecem um pessoal que s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>no</td>\n",
              "      <td>Podiam retirar dos lucros dos bancos</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id @@class                                           document\n",
              "0   1     yes               Votaram no PEZAO Agora tomem no CZAO\n",
              "1   2      no  cuidado com a poupanca pessoal Lembram o que a...\n",
              "2   3      no  Sabe o que eu acho engracado os nossos governa...\n",
              "3   4     yes  os cariocas tem o que merecem um pessoal que s...\n",
              "4   5      no              Podiam retirar dos lucros dos bancos "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uUCCWlDG52Nz",
        "colab_type": "text"
      },
      "source": [
        "#### 2.3.2 Dimensionalidade <a name=\"n3_dimensionalidade\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GBNG8DX052N0",
        "colab_type": "code",
        "colab": {},
        "outputId": "9ebe2b61-e9aa-4da0-cd3d-dd15c220eb2b"
      },
      "source": [
        "df.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1250, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UryLee_d52N1",
        "colab_type": "text"
      },
      "source": [
        "#### 2.3.3 Tipos de campos <a name=\"n3_tipos_campos\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pZgCYG7e52N2",
        "colab_type": "code",
        "colab": {},
        "outputId": "87438901-94ab-400f-c8fd-52d8f8fc18c9"
      },
      "source": [
        "df.info()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1250 entries, 0 to 1249\n",
            "Data columns (total 3 columns):\n",
            " #   Column    Non-Null Count  Dtype \n",
            "---  ------    --------------  ----- \n",
            " 0   id        1250 non-null   int64 \n",
            " 1   @@class   1250 non-null   object\n",
            " 2   document  1250 non-null   object\n",
            "dtypes: int64(1), object(2)\n",
            "memory usage: 29.4+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X3uX3z4552N4",
        "colab_type": "text"
      },
      "source": [
        "### 2.4. Análise exploratória <a name=\"n2_analise_exploratoria\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qNqXzGQY52N5",
        "colab_type": "text"
      },
      "source": [
        "#### 2.4.1 Contagem de sentenças por classificação <a name=\"n3_contagem_classificacao\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Ij-YRag52N5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 180
        },
        "outputId": "1ef67473-469b-4e86-a468-941610f67f60"
      },
      "source": [
        "offensive_count = df['@@class'].value_counts()\n",
        "print(offensive_count)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-09b89cc1ece4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0moffensive_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'@@class'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moffensive_count\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CEScBuO252N7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "outputId": "c09671ca-4625-483a-df60-d7666a1d681b"
      },
      "source": [
        "plt.figure(figsize=(8,4))\n",
        "plt.bar(offensive_count.index, offensive_count.values)\n",
        "plt.title(\"Distribuição de conteúdos positivos e negativos\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-fbc418c947f5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moffensive_count\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moffensive_count\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Distribuição de conteúdos positivos e negativos\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_dv-3TNX52N9",
        "colab_type": "text"
      },
      "source": [
        "#### 2.4.2 Redefinição da nomenclatura das colunas <a name=\"n3_redefinicao_colunas\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dx5vlQMa52N9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.columns = ['id', 'classification', 'speech']\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KBfFn8Iw52N_",
        "colab_type": "text"
      },
      "source": [
        "#### 2.4.3 Redefinição da coluna de classificação (0 e 1) <a name=\"n3_redefinicao_coluna_classificacao\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dDhSaaWi52N_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df['classification'] = df['classification'].replace(['no', 'yes'], [1, 0])\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KB7FEjwt52OB",
        "colab_type": "text"
      },
      "source": [
        "#### 2.4.4 Palavras mais comuns por categoria <a name=\"n3_palavras_comuns\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aOS48yze52OB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z3sPwJ3052OD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "wc = WordCloud(width = 800, height = 500, max_font_size = 110, collocations = False\n",
        "                          ).generate(' '.join(df[df.classification == 0].speech))\n",
        "\n",
        "plt.figure(figsize = (20, 10))\n",
        "plt.imshow(wc, interpolation = 'bilinear')\n",
        "plt.axis('off')\n",
        "plt.title('No Offensive', fontsize = 30)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yHLBVLWE52OF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "wc = WordCloud(width = 800, height = 500, max_font_size = 110, collocations = False\n",
        "                          ).generate(' '.join(df[df.classification == 1].speech))\n",
        "\n",
        "plt.figure(figsize = (20, 10))\n",
        "plt.imshow(wc, interpolation = 'bilinear')\n",
        "plt.axis('off')\n",
        "plt.title('Offensive', fontsize = 30)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-2S6K_l52OG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "words_list = [word for sentence in df.speech for word in sentence.split()]\n",
        "\n",
        "Counter(words_list).most_common(10)\n",
        "\n",
        "temp = pd.DataFrame(Counter(words_list).most_common(10))\n",
        "temp.columns = ['Words', 'Count']\n",
        "temp.style.background_gradient(cmap = 'Blues')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "djH0zri_52OI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "palavras_positivas = []\n",
        "palavras_negativas = []\n",
        "\n",
        "for index, row in df.iterrows():\n",
        "    if(row['ofensivo'] == 0):        \n",
        "        for x in row['texto'].split():            \n",
        "            palavras_negativas.append(x)\n",
        "    elif(row['ofensivo'] == 1):\n",
        "        for x in row['texto'].split():\n",
        "            palavras_positivas.append(x)\n",
        "   \n",
        "print(palavras_positivas)\n",
        "\n",
        "temp_pos = pd.DataFrame(Counter(palavras_positivas).most_common(10))\n",
        "temp_pos.columns = ['palavras_comum','counts']\n",
        "\n",
        "temp_neg = pd.DataFrame(Counter(palavras_negativas).most_common(10))\n",
        "temp_neg.columns = ['palavras_comum','counts']\n",
        "\n",
        "\n",
        "f, axes = plt.subplots(1,2,figsize=(18,10))\n",
        "sns.set(font_scale =1)\n",
        "sns.barplot(y= temp_pos.palavras_comum.values, x= temp_pos.counts.values,color=\"green\",ax=axes[0]).set_title(\"Positivo\",color=\"green\")\n",
        "sns.barplot(y= temp_neg.palavras_comum.values, x= temp_neg.counts.values,color=\"red\",ax=axes[1]).set_title(\"Negativo\",color=\"red\")\n",
        "\n",
        "plt.suptitle(\"Palavras mais comuns em cada categoria\" ,fontsize=25,color=\"blue\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VKgzvaQM6lPl",
        "colab_type": "text"
      },
      "source": [
        "#### 2.4.5 Frequência das palavras <a name=\"n3_frequencia_das_palavras\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pTU1_an26ySI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Analisando a frequência das palavras:\n",
        "import nltk\n",
        "from nltk import tokenize\n",
        "\n",
        "space_token = tokenize.WhitespaceTokenizer()\n",
        "\n",
        "all_words = ' '.join([word for word in df.sentence])\n",
        "frequence = nltk.FreqDist(space_token.tokenize(all_words))\n",
        "df_frequence = pd.DataFrame({ 'Word': list(frequence.keys()),'Frequence': list(frequence.values()) })\n",
        "                      \n",
        "df_frequence.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YoWbm3gS8-PA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Analisando as palavras com maiores frequências:\n",
        "df_frequence = df_frequence.nlargest(columns = \"Frequence\", n = 10)\n",
        "df_frequence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "srAfIaVs52OJ",
        "colab_type": "text"
      },
      "source": [
        "### 2.5. Técnicas de Pré Processamento <a name=\"n2_tecnicas_pre_processamento\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cmf5djhA52OK",
        "colab_type": "text"
      },
      "source": [
        "#### 2.5.1 Workflow NLP <a name=\"n3_workflow_nlp\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WD505WlW52OK",
        "colab_type": "text"
      },
      "source": [
        "<p style='text-align: center;'> \n",
        "    <img src='./assets/workflow_nlp.png'/>\n",
        "</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zx2Es0KZ52OK",
        "colab_type": "text"
      },
      "source": [
        "#### 2.5.2 Removendo stop words e pontuação <a name=\"n3_removendo_stop_words\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E7Pnhqyj52OL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Tratamento 1 (Stop Words):\n",
        "nltk.download('stopwords')\n",
        "stop_words = nltk.corpus.stopwords.words(\"portuguese\")\n",
        "processed_sentence = list()\n",
        "\n",
        "for sentence in df.sentence:\n",
        "    new_sentence = list()\n",
        "    \n",
        "    words = space_token.tokenize(sentence)\n",
        "    \n",
        "    for word in words:\n",
        "        if word not in stop_words:\n",
        "            new_sentence.append(word)\n",
        "    processed_sentence.append(' '.join(new_sentence))\n",
        "    \n",
        "df.sentence = processed_sentence\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vOqutCQl52OO",
        "colab_type": "text"
      },
      "source": [
        "#### 2.5.3 Pontuações <a name=\"n3_pontuacao\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y67sTQre_0Vm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Tratamento 02 (Removendo Pontuação):\n",
        "from string import punctuation\n",
        "punctuation_token = tokenize.WordPunctTokenizer()\n",
        "\n",
        "list_punctuation = list()\n",
        "for point in punctuation:\n",
        "    list_punctuation.append(point)\n",
        "\n",
        "punctuation_stopwords = list_punctuation + stop_words\n",
        "\n",
        "processed_sentence = list()\n",
        "\n",
        "for sentence in df.sentence:\n",
        "    new_sentence = list()\n",
        "    \n",
        "    words = punctuation_token.tokenize(sentence)\n",
        "    \n",
        "    for word in words:\n",
        "        if word not in punctuation_stopwords:\n",
        "            new_sentence.append(word)\n",
        "            \n",
        "    processed_sentence.append(' '.join(new_sentence))\n",
        "    \n",
        "df.sentence = processed_sentence\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EIc5jMZi52OQ",
        "colab_type": "text"
      },
      "source": [
        "#### 2.5.4 Normalização do texto <a name=\"n3_normalizacao_texto\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XuU07joZ_9TC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Tratamento 3 (Normalização do Texto):\n",
        "!pip install unidecode\n",
        "import unidecode\n",
        "\n",
        "without_accents = [unidecode.unidecode(word) for word in df.sentence]\n",
        "without_accents_stop_words = [unidecode.unidecode(word) for word in punctuation_stopwords]\n",
        "\n",
        "df.sentence = without_accents\n",
        "\n",
        "processed_sentence = list()\n",
        "\n",
        "for sentence in df.sentence:\n",
        "    new_sentence = list()\n",
        "    \n",
        "    words = punctuation_token.tokenize(sentence)\n",
        "    \n",
        "    for word in words:\n",
        "        if word not in without_accents_stop_words:\n",
        "            new_sentence.append(word)\n",
        "    \n",
        "    processed_sentence.append(' '.join(new_sentence))\n",
        "    \n",
        "df.sentence = processed_sentence\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gpsK5FzU52OT",
        "colab_type": "text"
      },
      "source": [
        "#### 2.5.5 Caracteres Maiúsculos <a name=\"n3_caracteres_maiusculo\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AC-V9QeeAIzr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Tratamento 4 (Letras Maiúsculas):\n",
        "processed_sentence = list()\n",
        "\n",
        "for sentence in df.sentence:\n",
        "    new_sentence = list()\n",
        "    \n",
        "    sentence = sentence.lower()\n",
        "    words = punctuation_token.tokenize(sentence)\n",
        "    \n",
        "    for word in words:\n",
        "        if word not in without_accents_stop_words:\n",
        "            new_sentence.append(word)\n",
        "    \n",
        "    processed_sentence.append(' '.join(new_sentence))\n",
        "    \n",
        "df.sentence = processed_sentence\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p4fNi8nM52OU",
        "colab_type": "text"
      },
      "source": [
        "#### 2.5.6 Stemming <a name=\"n3_stemming\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hcXy4t1mAND_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Tratamento 5 (Stemming):\n",
        "import nltk\n",
        "nltk.download('rslp')\n",
        "\n",
        "stemmer = nltk.RSLPStemmer()\n",
        "\n",
        "processed_sentence = list()\n",
        "\n",
        "for sentence in df.sentence:\n",
        "    new_sentence = list()\n",
        "    \n",
        "    words = punctuation_token.tokenize(sentence)\n",
        "    \n",
        "    for word in words:\n",
        "        if word not in without_accents_stop_words:\n",
        "            new_sentence.append(stemmer.stem(word))\n",
        "    processed_sentence.append(' '.join(new_sentence))\n",
        "    \n",
        "df.sentence = processed_sentence\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_wKh3gGy52OW",
        "colab_type": "text"
      },
      "source": [
        "#### 2.5.7 Word Cloud <a name=\"n3_word_cloud\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0HT0T7lGAhu3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Apresentando Word Cloud:\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%matplotlib inline\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "positive_text = df.query('classification == 0')\n",
        "all_words = ' '.join([word for word in positive_text['sentence']])\n",
        "\n",
        "word_cloud = WordCloud(width = 800, height = 500,\n",
        "                            max_font_size=110, collocations=False).generate(all_words)\n",
        "plt.figure(figsize=(10,7))\n",
        "plt.imshow(word_cloud, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gf3GmwAsAky2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "negative_text = df.query('classification == 1')\n",
        "all_words = ' '.join([word for word in negative_text['sentence']])\n",
        "\n",
        "word_cloud = WordCloud(width = 800, height = 500,\n",
        "                            max_font_size=110, collocations=False).generate(all_words)\n",
        "plt.figure(figsize=(10,7))\n",
        "plt.imshow(word_cloud, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DWK1PWgz52OY",
        "colab_type": "text"
      },
      "source": [
        "#### 2.5.8 Frequência das Palavras<a name=\"n3_frequencia_palavras_pareto\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d6sgitZaAv7c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Gerando um gráfico de Pareto:\n",
        "all_words = ' '.join([word for word in df.sentence])\n",
        "frequence = nltk.FreqDist(space_token.tokenize(all_words))\n",
        "\n",
        "df_frequence = pd.DataFrame({ 'Word': list(frequence.keys()), 'Frequence': list(frequence.values()) })\n",
        "df_frequence = df_frequence.nlargest(columns = \"Frequence\", n = 20)\n",
        "\n",
        "plt.figure(figsize=(12,8))\n",
        "ax = sns.barplot(data = df_frequence, x= \"Word\", y = \"Frequence\", color = 'gray')\n",
        "ax.set(ylabel = \"Count\")\n",
        "plt.show()   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8HdHBFnD52Oc",
        "colab_type": "text"
      },
      "source": [
        "### 2.6. Classificadores <a name=\"n2_classificadores\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6dbeXMsO52Oc",
        "colab_type": "text"
      },
      "source": [
        "#### 2.6.1 TF-IDF / Balanceamento <a name=\"n3_tf_idf_balanceamento\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gOOkJk8uBYg-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Para esse ponto, considera-se que as técnicas de pré-processamento foram realizadas. Aqui, será utilizado o classificador\n",
        "\n",
        "tfidf = TfidfVectorizer();\n",
        "# 1.Separar os dados de X e y:\n",
        "X = tfidf.fit_transform(df.sentence)\n",
        "y = df.classification"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ZTJp8MEBYrf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 2. Executar o train_test_split com stratify:\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 42, stratify = y)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bS5pOHwiBcop",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 3. Normalizar o desbalanceamento com o Smote: \n",
        "sm = SMOTE(random_state = 42, sampling_strategy = 'minority')\n",
        "X_train_res, y_train_res = sm.fit_resample(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iitMrLs052Oh",
        "colab_type": "text"
      },
      "source": [
        "#### 2.6.2 Global Class <a name=\"n3_global_class\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b9rWXgv_CMUW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJfDzv_z52Oq",
        "colab_type": "text"
      },
      "source": [
        "#### 2.6.3 Logistic Regression <a name=\"n3_logistic_regression\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ozBk_DlZCMrc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CqKlArr352Os",
        "colab_type": "text"
      },
      "source": [
        "#### 2.6.4 Gaussian Naive Bayes <a name=\"n3_gaussian_nb\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O_CzYO3-CNHx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Processar o GaussianNB \n",
        "classifier = GaussianNB();\n",
        "classifier.fit(X_train_res.toarray(), y_train_res)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mYPUool-C1p9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Apresentar os resultados de predict \n",
        "y_pred = classifier.predict(X_test.toarray())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UuymVTUVC1sO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Obtendo a F Medida  \n",
        "score = f1_score(y_test, y_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XZHUR7qVC1uh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"A F Medida do GaussianNB foi %.2f%%\" % score)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QUyEkvxiC1w2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Relatórios:\n",
        "print('Classification Reports: ')\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EC2RYUZIC1zM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lVfPnSLCC11a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hhQcs4zVC13J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3-EDdXrn52Ox",
        "colab_type": "text"
      },
      "source": [
        "#### 2.6.5 Random Forest <a name=\"n3_random_forest\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B2CIkHP3CNiA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Processar um GridSearchCV para escolha da melhor configuração:\n",
        "params = {'n_estimators' : [int(x) for x in np.linspace (start = 10, stop = 100, num = 10)], #Número de árvores na floresta.\n",
        "          'max_features' : ['auto', 'sqrt'], #Número de recursos a serem considerados em cada divisão\n",
        "          'max_depth': [2,5]#Profundidade máxima da árvore.          \n",
        "           #número mínimo de amostras que devem estar presentes no nó folha após a divisão de um nó\n",
        "          }\n",
        "\n",
        "search = GridSearchCV(RandomForestClassifier(), params, cv = 10, scoring='f1_micro', n_jobs=4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YsXbefV9ChX6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "search.fit(X_train_res, y_train_res)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sVBR99ANChaY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "search.best_params_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M8Ekth-iChdZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "results = pd.DataFrame(search.cv_results_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DecsP3haChfk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Apresentar os resultados de acordo com as configurações:\n",
        "print(results)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NrOUDcIuChh2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred = search.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EF6pcwKdCqhR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Obtendo a F Medida (verificar como calcular F Medida):\n",
        "#F Medida\n",
        "fmd = f1_score(y_test, y_pred)\n",
        "print(\"A F Medida do Grid foi %.2f%%\" % fmd)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FNuc4s7OCqjm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Apresentar a melhor configuração:\n",
        "print(search.best_estimator_)\n",
        "print('Score do GridSearchCV (melhor): ', search.best_score_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cgI-kmw7Cql-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Relatórios:\n",
        "print('Classification Reports: ')\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-SUp3RSFChkI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('Matriz de Confusão: ')\n",
        "print(pd.crosstab(y_test, y_pred, rownames=['Real'], colnames=['Predito'], margins=True))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y7VArrvC52Oy",
        "colab_type": "text"
      },
      "source": [
        "#### 2.6.6 Regressão Logística Multinomial <a name=\"n3_regressao_logistica_multinomial\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f4_whlq-CQe0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "78I0TMDR52O0",
        "colab_type": "text"
      },
      "source": [
        "#### 2.6.7 (...) <a name=\"n3_\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p5-v55U152O1",
        "colab_type": "text"
      },
      "source": [
        "(conteúdo)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ye1TlG4k52O2",
        "colab_type": "text"
      },
      "source": [
        "### 2.7 Geração dos modelos (armazenamento) <a name=\"n2_geracao_modelos\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DfnvrZkcCWJ9",
        "colab_type": "text"
      },
      "source": [
        "(conteúdo)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4EsMdrwVCB_T",
        "colab_type": "text"
      },
      "source": [
        "### 2.8 SPA de Classificação <a name=\"n2_spa_classificacao\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jAuxG4OE52O2",
        "colab_type": "text"
      },
      "source": [
        "(conteúdo)"
      ]
    }
  ]
}